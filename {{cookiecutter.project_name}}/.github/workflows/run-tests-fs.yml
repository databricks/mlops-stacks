name: Feature and Training Integration Tests for {{cookiecutter.project_name}}
on:
  workflow_dispatch:
  pull_request:
    paths-ignore:
      - 'databricks-config/**'

env:
  DATABRICKS_HOST: {{cookiecutter.databricks_staging_workspace_host}}
  NODE_TYPE_ID: {{cookiecutter.cloud_specific_node_type_id}}
  {% if cookiecutter.cloud == "aws" -%}
  DATABRICKS_TOKEN: {% raw %}${{secrets.STAGING_WORKSPACE_TOKEN}}{% endraw %}
  {%- endif %}

concurrency: {{cookiecutter.project_name}}-feature-training-integration-test-staging

jobs:
  unit_tests:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - uses: actions/setup-python@v2
        with:
          python-version: 3.8
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -r test-requirements.txt
      - name: Run tests with pytest
        run: pytest
  
  integration_test:
    needs: unit_tests
    runs-on: ubuntu-20.04
    steps:
      - name: Checkout repo
        uses: actions/checkout@v3
        {%- if cookiecutter.cloud == "azure" %}
      - name: Generate AAD Token
        {% raw %}run: ./.github/workflows/scripts/generate-aad-token.sh ${{ secrets.stagingAzureSpTenantId }} ${{ secrets.stagingAzureSpApplicationId }} ${{ secrets.stagingAzureSpClientSecret }}{% endraw %}
        {%- endif %}
      # This step populates a JSON Databricks job payload that will be submitted as an integration test run.
      # It currently builds a one-off multi-task job that contains feature engineering tasks to populate Feature
      # Store tables, and a training task that uses those tables. 
      # You will need to modify the contents below to fit your pipelines (both # of tasks and input parameters for each
      # task).
      - name: Build JSON job payload for integration test
        uses: actions/github-script@v6
        id: integration-test-content
        with:
          # TODO update the tasks and notebook parameters below to match your integration test setup.
          script: |
            const output = `
                    {
            "run_name": "features-training-integration-test",
            "tasks": [
              {
                "task_key": "pickup-features",
                "notebook_task": {
                  "notebook_path": "notebooks/GenerateAndWriteFeatures", 
                  "base_parameters": {
                    "env": "staging",
                    "test_mode": "True",
                    "input_table_path": "/databricks-datasets/nyctaxi-with-zipcodes/subsampled",
                    "timestamp_column": "tpep_pickup_datetime",
                    "output_table_name": "feature_store_taxi_example.trip_pickup_features_test",
                    "features_transform_module": "pickup_features",
                    "primary_keys": "zip"
                  }
                },
                "new_cluster": {
                  "spark_version": "11.0.x-cpu-ml-scala2.12",
                  {% raw %}"node_type_id": "${{ env.NODE_TYPE_ID }}",{% endraw %}
                  "num_workers": 0,
                  "spark_conf": {
                    "spark.databricks.cluster.profile": "singleNode",
                    "spark.master": "local[*, 4]"
                  },
                  "custom_tags": {
                    "ResourceClass": "SingleNode",
                    "clusterSource": "mlops-stack/0.0"
                  }
                }
              },
              {
                "task_key": "dropoff-features",
                "notebook_task": {
                  "notebook_path": "notebooks/GenerateAndWriteFeatures",
                  "base_parameters": {                    
                    "env": "staging",
                    "test_mode": "True",
                    "input_table_path": "/databricks-datasets/nyctaxi-with-zipcodes/subsampled",
                    "timestamp_column": "tpep_dropoff_datetime",
                    "output_table_name": "feature_store_taxi_example.trip_dropoff_features_test",
                    "features_transform_module": "dropoff_features",
                    "primary_keys": "zip"
                  }
                },
                "new_cluster": {
                  "spark_version": "11.0.x-cpu-ml-scala2.12",
                  {% raw %}"node_type_id": "${{ env.NODE_TYPE_ID }}",{% endraw %}
                  "num_workers": 0,
                  "spark_conf": {
                    "spark.databricks.cluster.profile": "singleNode",
                    "spark.master": "local[*, 4]"
                  },
                  "custom_tags": {
                    "ResourceClass": "SingleNode",
                    "clusterSource": "mlops-stack/0.0"
                  }
                }
              },
              {
                "task_key": "training",
                "depends_on": [
                  {
                    "task_key": "dropoff-features"
                  },
                  {
                    "task_key": "pickup-features"
                  }
                ],
                "notebook_task": {
                  "notebook_path": "notebooks/TrainWithFeatureStore",
                  "base_parameters": {
                    "env": "staging",
                    "test_mode": "True",
                    "training_data_path": "/databricks-datasets/nyctaxi-with-zipcodes/subsampled",
                    "experiment_name": "{{cookiecutter.mlflow_experiment_parent_dir}}/{{cookiecutter.experiment_base_name}}-test",
                    "model_name": "{{cookiecutter.model_name}}-test"
                  }
                },
                "new_cluster": {
                  "spark_version": "11.0.x-cpu-ml-scala2.12",
                  {% raw %}"node_type_id": "${{ env.NODE_TYPE_ID }}",{% endraw %}
                  "num_workers": 0,
                  "spark_conf": {
                    "spark.databricks.cluster.profile": "singleNode",
                    "spark.master": "local[*, 4]"
                  },
                  "custom_tags": {
                    "ResourceClass": "SingleNode",
                    "clusterSource": "mlops-stack/0.0"
                  }
                }
              }
            ],
            "git_source": {
              "git_url": {% raw %}"${{ github.server_url }}/${{ github.repository }}",{% endraw %}
              "git_provider": "gitHub",
              "git_commit": {% raw %}"${{ github.event.pull_request.head.sha || github.sha }}"{% endraw %}
            },
            "access_control_list": [
              {
                "group_name": "users",
                "permission_level": "CAN_VIEW"
              }
            ]
            }`
            return output.replace(/\r?\n|\r/g, '')    
      - name: Feature Store/Model Training Integration Test
        id: features-training-integration-test
        run: |
          python -m pip install --upgrade pip
          pip install databricks-cli
          databricks jobs configure --version=2.1
          echo {% raw %}${{steps.integration-test-content.outputs.result}} > test.json {% endraw %}
          databricks runs submit --json-file test.json --wait > tmp-output.json
          # We want to extract the run id as it's useful to show in the Github UI (as a comment).
          head -3  tmp-output.json  | jq '.run_id'  > run-id.json
          databricks runs get --run-id "{% raw %}$(cat run-id.json){% endraw %}" | jq -r '.run_page_url' > run-page-url.json
          echo "run-url={% raw %}$(cat run-page-url.json){% endraw %}" >> "$GITHUB_OUTPUT"
      - name: Create Comment with Training Model Output
        uses: actions/github-script@v6
        id: comment
        with:
          github-token: {% raw %}${{ secrets.GITHUB_TOKEN }}{% endraw %}
          script: |
            const output = `
            The training integration test run is available [here]({% raw %}${{ steps.features-training-integration-test.outputs.run-url }}{% endraw %}).`

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: output
            })
