name: ML Code Tests for {{cookiecutter.project_name}}
on:
  workflow_dispatch:
  pull_request:
    paths-ignore:
      - 'databricks-config/**'

env:
  DATABRICKS_HOST: {{cookiecutter.databricks_staging_workspace_host}}
  NODE_TYPE_ID: {{ cookiecutter.cloud_specific_node_type_id}}
  {% if cookiecutter.cloud == "aws" -%}
  DATABRICKS_TOKEN: {% raw %}${{secrets.STAGING_WORKSPACE_TOKEN}}{% endraw %}
  {%- endif %}
  
jobs:
  unit_tests:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - uses: actions/setup-python@v2
        with:
          python-version: 3.8
       {%- if cookiecutter.include_feature_store == "yes" %}
      # Feature store tests bring up a local Spark session, so Java is required.
      - uses: actions/setup-java@v2
        with:
          distribution: 'temurin'
          java-version: '11'
          {%- endif %}
      - name: Install dependencies
        run: |
            python -m pip install --upgrade pip
            pip install -r requirements.txt
            pip install -r test-requirements.txt
      - name: Run tests with pytest
        run: pytest
  
  integration_test:
    needs: unit_tests
    runs-on: ubuntu-20.04
    steps:
      - name: Checkout repo
        uses: actions/checkout@v3
        {%- if cookiecutter.cloud == "azure" %}
      - name: Generate AAD Token
        {% raw %}run: ./.github/workflows/scripts/generate-aad-token.sh ${{ secrets.stagingAzureSpTenantId }} ${{ secrets.stagingAzureSpApplicationId }} ${{ secrets.stagingAzureSpClientSecret }}{% endraw %}
        {%- endif %}
      - name: Train model
        uses: databricks/run-notebook@v0
        id: train
        with:
          {%- if cookiecutter.include_feature_store == "yes" %}
          local-notebook-path: notebooks/TrainWithFeatureStore.py
          {%- else %}
          local-notebook-path: notebooks/Train.py
          {%- endif %}
          {% raw %}git-commit: ${{ github.event.pull_request.head.sha || github.sha }}{% endraw %}
          new-cluster-json: >
            {
              "spark_version": "11.0.x-cpu-ml-scala2.12",
              {% raw %}"node_type_id": "${{ env.NODE_TYPE_ID }}",{% endraw %}
              "num_workers": 0,
              "spark_conf": {
                "spark.databricks.cluster.profile": "singleNode",
                "spark.master": "local[*, 4]"
              },
              "custom_tags": {
                "ResourceClass": "SingleNode",
                "clusterSource": "mlops-stack/0.0"
              }
            }
          access-control-list-json: >
            [
              {
                "group_name": "{{cookiecutter.read_user_group}}",
                "permission_level": "CAN_VIEW"
              }
            ]
          run-name: {{ cookiecutter.project_name}} Integration Test
          notebook-params-json: >
            {
              "env": "staging",
              "test_mode": "True"
            }
          {% raw %}pr-comment-github-token: ${{ secrets.GITHUB_TOKEN }}{% endraw %}
