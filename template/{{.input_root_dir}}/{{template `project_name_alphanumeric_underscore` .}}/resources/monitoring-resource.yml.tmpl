# Please complete all the TODOs in this file.
# The regression monitor defined here works OOB with this example regression notebook: {{ template `generate_doc_link` (map (pair "cloud" .input_cloud) (pair "path" "_extras/notebooks/source/monitoring/regression-monitor.html")) }}
# NOTE: Monitoring only works on Unity Catalog tables.

new_cluster: &new_cluster
  new_cluster:
    num_workers: 3
    spark_version: 15.3.x-cpu-ml-scala2.12
    node_type_id: {{template `cloud_specific_node_type_id` .}}
    data_security_mode: "SINGLE_USER"
    custom_tags:
      clusterSource: mlops-stacks_{{template `stacks_version` .}}

common_permissions: &permissions
  permissions:
    - level: CAN_VIEW
      group_name: users

resources:
  quality_monitors:
    {{ .input_project_name }}_quality_monitor:
      table_name: {{ .input_inference_table_name }}
      # TODO: Update the output schema name as per your requirements
      output_schema_name: ${bundle.target}.{{ .input_project_name }}
      # TODO: Update the below parameters as per your requirements
      assets_dir: /Users/${workspace.current_user.userName}/databricks_lakehouse_monitoring
      inference_log:
        granularities: [1 day]
        model_id_col: model_id
        prediction_col: prediction
        label_col: price
        problem_type: PROBLEM_TYPE_REGRESSION
        timestamp_col: timestamp
      schedule:
        quartz_cron_expression: 0 0 8 * * ? # Run Every day at 8am
        timezone_id: UTC
  jobs:
    retraining_job:
      name: ${bundle.target}-{{ .input_project_name }}-monitoring-retraining-job
      tasks:
        - task_key: monitored_metric_violation_check
          <<: *new_cluster
          notebook_task:
            notebook_path: ../monitoring/notebooks/MonitoredMetricViolationCheck.py
            base_parameters:
              env: ${bundle.target}
              table_name_under_monitor: {{ .input_inference_table_name }}
              # TODO: Update the metric to be monitored and violation threshold
              metric_to_monitor: root_mean_squared_error
              metric_violation_threshold: 100
              num_evaluation_windows: 5
              num_violation_windows: 2
              # git source information of current ML resource deployment. It will be persisted as part of the workflow run
              git_source_info: url:${bundle.git.origin_url}; branch:${bundle.git.branch}; commit:${bundle.git.commit}

        - task_key: is_metric_violated
          depends_on:
            - task_key: monitored_metric_violation_check
          condition_task:
            op: EQUAL_TO
            left: "{{"{{tasks.monitored_metric_violation_check.values.is_metric_violated}}"}}"
            right: "true"
          git_source_info: url:${bundle.git.origin_url}; branch:${bundle.git.branch}; commit:${bundle.git.commit}

        - task_key: trigger_retraining
          depends_on:
            - task_key: is_metric_violated
              outcome: "true"
          run_job_task:
            job_id: ${resources.jobs.model_training_job.id}
            git_source_info: url:${bundle.git.origin_url}; branch:${bundle.git.branch}; commit:${bundle.git.commit}

      schedule:
        quartz_cron_expression: "0 0 18 * * ?" # daily at 6pm
        timezone_id: UTC
      <<: *permissions
      # If you want to turn on notifications for this job, please uncomment the below code,
      # and provide a list of emails to the on_failure argument.
      #
      #  email_notifications:
      #    on_failure:
      #      - first@company.com
      #      - second@company.com

